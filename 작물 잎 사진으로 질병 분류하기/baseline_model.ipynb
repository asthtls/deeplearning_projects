{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[WinError 183] 파일이 이미 있으므로 만들 수 없습니다: './splitted/'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15612\\1032287516.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mbase_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'./splitted/'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mtrain_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'train'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileExistsError\u001b[0m: [WinError 183] 파일이 이미 있으므로 만들 수 없습니다: './splitted/'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    " \n",
    "original_dataset_dir = './dataset/'   \n",
    "classes_list = os.listdir(original_dataset_dir) \n",
    " \n",
    "base_dir = './splitted/' \n",
    "os.mkdir(base_dir)\n",
    " \n",
    "train_dir = os.path.join(base_dir, 'train') \n",
    "os.mkdir(train_dir)\n",
    "validation_dir = os.path.join(base_dir, 'val')\n",
    "os.mkdir(validation_dir)\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "os.mkdir(test_dir)\n",
    "\n",
    "for cls in classes_list:     \n",
    "    os.mkdir(os.path.join(train_dir, cls))\n",
    "    os.mkdir(os.path.join(validation_dir, cls))\n",
    "    os.mkdir(os.path.join(test_dir, cls))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size( Apple___Apple_scab ):  378\n",
      "Validation size( Apple___Apple_scab ):  126\n",
      "Test size( Apple___Apple_scab ):  126\n",
      "Train size( Apple___Black_rot ):  372\n",
      "Validation size( Apple___Black_rot ):  124\n",
      "Test size( Apple___Black_rot ):  124\n",
      "Train size( Apple___Cedar_apple_rust ):  165\n",
      "Validation size( Apple___Cedar_apple_rust ):  55\n",
      "Test size( Apple___Cedar_apple_rust ):  55\n",
      "Train size( Apple___healthy ):  987\n",
      "Validation size( Apple___healthy ):  329\n",
      "Test size( Apple___healthy ):  329\n",
      "Train size( Cherry___healthy ):  512\n",
      "Validation size( Cherry___healthy ):  170\n",
      "Test size( Cherry___healthy ):  170\n",
      "Train size( Cherry___Powdery_mildew ):  631\n",
      "Validation size( Cherry___Powdery_mildew ):  210\n",
      "Test size( Cherry___Powdery_mildew ):  210\n",
      "Train size( Corn___Cercospora_leaf_spot Gray_leaf_spot ):  307\n",
      "Validation size( Corn___Cercospora_leaf_spot Gray_leaf_spot ):  102\n",
      "Test size( Corn___Cercospora_leaf_spot Gray_leaf_spot ):  102\n",
      "Train size( Corn___Common_rust ):  715\n",
      "Validation size( Corn___Common_rust ):  238\n",
      "Test size( Corn___Common_rust ):  238\n",
      "Train size( Corn___healthy ):  697\n",
      "Validation size( Corn___healthy ):  232\n",
      "Test size( Corn___healthy ):  232\n",
      "Train size( Corn___Northern_Leaf_Blight ):  591\n",
      "Validation size( Corn___Northern_Leaf_Blight ):  197\n",
      "Test size( Corn___Northern_Leaf_Blight ):  197\n",
      "Train size( Grape___Black_rot ):  708\n",
      "Validation size( Grape___Black_rot ):  236\n",
      "Test size( Grape___Black_rot ):  236\n",
      "Train size( Grape___Esca_(Black_Measles) ):  829\n",
      "Validation size( Grape___Esca_(Black_Measles) ):  276\n",
      "Test size( Grape___Esca_(Black_Measles) ):  276\n",
      "Train size( Grape___healthy ):  253\n",
      "Validation size( Grape___healthy ):  84\n",
      "Test size( Grape___healthy ):  84\n",
      "Train size( Grape___Leaf_blight_(Isariopsis_Leaf_Spot) ):  645\n",
      "Validation size( Grape___Leaf_blight_(Isariopsis_Leaf_Spot) ):  215\n",
      "Test size( Grape___Leaf_blight_(Isariopsis_Leaf_Spot) ):  215\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    " \n",
    "for cls in classes_list:\n",
    "    path = os.path.join(original_dataset_dir, cls)\n",
    "    fnames = os.listdir(path)\n",
    " \n",
    "    train_size = math.floor(len(fnames) * 0.6)\n",
    "    validation_size = math.floor(len(fnames) * 0.2)\n",
    "    test_size = math.floor(len(fnames) * 0.2)\n",
    "    \n",
    "    train_fnames = fnames[:train_size]\n",
    "    print(\"Train size(\",cls,\"): \", len(train_fnames))\n",
    "    for fname in train_fnames:\n",
    "        src = os.path.join(path, fname)\n",
    "        dst = os.path.join(os.path.join(train_dir, cls), fname)\n",
    "        shutil.copyfile(src, dst)\n",
    "        \n",
    "    validation_fnames = fnames[train_size:(validation_size + train_size)]\n",
    "    print(\"Validation size(\",cls,\"): \", len(validation_fnames))\n",
    "    for fname in validation_fnames:\n",
    "        src = os.path.join(path, fname)\n",
    "        dst = os.path.join(os.path.join(validation_dir, cls), fname)\n",
    "        shutil.copyfile(src, dst)\n",
    "        \n",
    "    test_fnames = fnames[(train_size+validation_size):(validation_size + train_size +test_size)]\n",
    "\n",
    "    print(\"Test size(\",cls,\"): \", len(test_fnames))\n",
    "    for fname in test_fnames:\n",
    "        src = os.path.join(path, fname)\n",
    "        dst = os.path.join(os.path.join(test_dir, cls), fname)\n",
    "        shutil.copyfile(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline model \n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device('cuda' if USE_CUDA else 'cpu')\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "EPOCH = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_base = transforms.Compose([transforms.Resize((64,64)),\n",
    "                                    transforms.ToTensor()])\n",
    "\n",
    "train_dataset = ImageFolder(root='./splitted/', transform=transform_base)\n",
    "val_dataset = ImageFolder(root='./splitted/val/', transform=transform_base)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 설계\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 64, 3, padding=1)\n",
    "\n",
    "        self.fc1 = nn.Linear(4096, 512)\n",
    "        self.fc2 = nn.Linear(512, 33)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = F.dropout(x, p=0.25, training=self.training)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = F.dropout(x, p=0.25, training=self.training)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = F.dropout(x, p=0.25, training=self.training)\n",
    "\n",
    "        x = x.view(-1, 4096)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_base = Net().to(DEVICE)\n",
    "optimizer = optim.Adam(model_base.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 함수\n",
    "def train(model, train_loader, optimizer):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 평가 함수\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "            output = model(data)\n",
    "\n",
    "            test_loss += F.cross_entropy(output, target, reduction='sum').item()\n",
    "            \n",
    "            pred = output.max(1, keepdim=True)[1]\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return test_loss, test_accuracy\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------epoch 1 -------------\n",
      "train loss : 1.0879, Accuracy : 60.02%\n",
      "val loss : 5.1166, Accuracy : 4.78%\n",
      "Completed in 0m 57s\n",
      "--------------epoch 2 -------------\n",
      "train loss : 1.0326, Accuracy : 60.02%\n",
      "val loss : 5.6365, Accuracy : 4.78%\n",
      "Completed in 0m 48s\n",
      "--------------epoch 3 -------------\n",
      "train loss : 0.9937, Accuracy : 60.02%\n",
      "val loss : 6.4137, Accuracy : 4.78%\n",
      "Completed in 0m 49s\n",
      "--------------epoch 4 -------------\n",
      "train loss : 1.0033, Accuracy : 60.02%\n",
      "val loss : 6.1748, Accuracy : 4.78%\n",
      "Completed in 0m 51s\n",
      "--------------epoch 5 -------------\n",
      "train loss : 0.9909, Accuracy : 60.02%\n",
      "val loss : 6.5117, Accuracy : 4.78%\n",
      "Completed in 0m 50s\n",
      "--------------epoch 6 -------------\n",
      "train loss : 0.9708, Accuracy : 60.02%\n",
      "val loss : 7.1682, Accuracy : 4.78%\n",
      "Completed in 0m 48s\n",
      "--------------epoch 7 -------------\n",
      "train loss : 0.9712, Accuracy : 60.02%\n",
      "val loss : 7.2628, Accuracy : 4.78%\n",
      "Completed in 0m 49s\n",
      "--------------epoch 8 -------------\n",
      "train loss : 0.9684, Accuracy : 60.02%\n",
      "val loss : 7.4731, Accuracy : 4.78%\n",
      "Completed in 0m 48s\n",
      "--------------epoch 9 -------------\n",
      "train loss : 0.9748, Accuracy : 60.02%\n",
      "val loss : 7.4256, Accuracy : 4.78%\n",
      "Completed in 0m 48s\n",
      "--------------epoch 10 -------------\n",
      "train loss : 0.9686, Accuracy : 60.02%\n",
      "val loss : 7.7299, Accuracy : 4.78%\n",
      "Completed in 0m 48s\n",
      "--------------epoch 11 -------------\n",
      "train loss : 0.9634, Accuracy : 60.02%\n",
      "val loss : 7.9343, Accuracy : 4.78%\n",
      "Completed in 0m 49s\n",
      "--------------epoch 12 -------------\n",
      "train loss : 0.9584, Accuracy : 60.02%\n",
      "val loss : 8.4420, Accuracy : 4.78%\n",
      "Completed in 0m 48s\n",
      "--------------epoch 13 -------------\n",
      "train loss : 0.9563, Accuracy : 60.02%\n",
      "val loss : 8.4469, Accuracy : 4.78%\n",
      "Completed in 0m 48s\n",
      "--------------epoch 14 -------------\n",
      "train loss : 0.9620, Accuracy : 60.02%\n",
      "val loss : 8.4198, Accuracy : 4.78%\n",
      "Completed in 0m 49s\n",
      "--------------epoch 15 -------------\n",
      "train loss : 0.9509, Accuracy : 60.02%\n",
      "val loss : 9.0260, Accuracy : 4.78%\n",
      "Completed in 0m 48s\n",
      "--------------epoch 16 -------------\n",
      "train loss : 0.9570, Accuracy : 60.04%\n",
      "val loss : 8.9168, Accuracy : 4.78%\n",
      "Completed in 0m 48s\n",
      "--------------epoch 17 -------------\n",
      "train loss : 0.9489, Accuracy : 60.02%\n",
      "val loss : 9.5728, Accuracy : 4.78%\n",
      "Completed in 0m 49s\n",
      "--------------epoch 18 -------------\n",
      "train loss : 0.9497, Accuracy : 60.02%\n",
      "val loss : 9.5099, Accuracy : 4.78%\n",
      "Completed in 0m 52s\n",
      "--------------epoch 19 -------------\n",
      "train loss : 0.9491, Accuracy : 60.02%\n",
      "val loss : 9.7817, Accuracy : 4.78%\n",
      "Completed in 0m 52s\n",
      "--------------epoch 20 -------------\n",
      "train loss : 0.9515, Accuracy : 60.09%\n",
      "val loss : 9.3790, Accuracy : 4.78%\n",
      "Completed in 0m 52s\n",
      "--------------epoch 21 -------------\n",
      "train loss : 0.9500, Accuracy : 60.06%\n",
      "val loss : 9.1382, Accuracy : 4.78%\n",
      "Completed in 0m 54s\n",
      "--------------epoch 22 -------------\n",
      "train loss : 0.9479, Accuracy : 60.02%\n",
      "val loss : 9.5456, Accuracy : 4.78%\n",
      "Completed in 0m 52s\n",
      "--------------epoch 23 -------------\n",
      "train loss : 0.9441, Accuracy : 60.07%\n",
      "val loss : 10.2680, Accuracy : 4.78%\n",
      "Completed in 0m 51s\n",
      "--------------epoch 24 -------------\n",
      "train loss : 0.9530, Accuracy : 60.04%\n",
      "val loss : 9.6576, Accuracy : 4.78%\n",
      "Completed in 0m 52s\n",
      "--------------epoch 25 -------------\n",
      "train loss : 0.9440, Accuracy : 60.03%\n",
      "val loss : 10.0247, Accuracy : 4.78%\n",
      "Completed in 0m 52s\n",
      "--------------epoch 26 -------------\n",
      "train loss : 0.9422, Accuracy : 60.17%\n",
      "val loss : 10.2984, Accuracy : 4.78%\n",
      "Completed in 0m 48s\n",
      "--------------epoch 27 -------------\n",
      "train loss : 0.9427, Accuracy : 60.03%\n",
      "val loss : 10.2870, Accuracy : 4.78%\n",
      "Completed in 0m 52s\n",
      "--------------epoch 28 -------------\n",
      "train loss : 0.9435, Accuracy : 60.09%\n",
      "val loss : 10.4527, Accuracy : 4.78%\n",
      "Completed in 0m 50s\n",
      "--------------epoch 29 -------------\n",
      "train loss : 0.9392, Accuracy : 60.28%\n",
      "val loss : 9.9518, Accuracy : 4.78%\n",
      "Completed in 0m 52s\n",
      "--------------epoch 30 -------------\n",
      "train loss : 0.9426, Accuracy : 60.35%\n",
      "val loss : 10.1262, Accuracy : 4.78%\n",
      "Completed in 0m 50s\n"
     ]
    }
   ],
   "source": [
    "# 모델 학습 실행\n",
    "\n",
    "import time\n",
    "import copy \n",
    "\n",
    "def train_baseline(model, train_loader, val_loader, optimizer, num_epochs=30):\n",
    "    best_acc = 0.0\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    \n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        since = time.time()\n",
    "        train(model, train_loader, optimizer)\n",
    "        train_loss, train_acc = evaluate(model, train_loader)\n",
    "        val_loss, val_acc = evaluate(model, val_loader)\n",
    "\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        print('--------------epoch {} -------------'.format(epoch))\n",
    "        print('train loss : {:.4f}, Accuracy : {:.2f}%'.format(train_loss,  train_acc))\n",
    "        print('val loss : {:.4f}, Accuracy : {:.2f}%'.format(val_loss,  val_acc))\n",
    "        print('Completed in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed%60))\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n",
    "\n",
    "base = train_baseline(model_base, train_loader, val_loader, optimizer, EPOCH)\n",
    "\n",
    "torch.save(base, 'baseline.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfer learning\n",
    "\n",
    "data_transforms = {\n",
    "    'train':transforms.Compose([\n",
    "        transforms.Resize([64,64]),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.RandomCrop(52),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456, 0.406], [0.229,0.224,0.225])\n",
    "    ]),\n",
    "    'val':transforms.Compose([\n",
    "        transforms.Resize([64,64]),\n",
    "        transforms.RandomCrop(52),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456, 0.406], [0.229,0.224,0.225])\n",
    "    ])\n",
    "\n",
    "}\n",
    "\n",
    "data_dir = './splitted'\n",
    "image_datasets = {x : ImageFolder(root=os.path.join(data_dir, x), transform=data_transforms[x]) for x in ['train','val']}\n",
    "\n",
    "dataloaders = {x:torch.utils.data.DataLoader(image_datasets[x], batch_size=BATCH_SIZE, shuffle=True, num_workers=4) for x in ['train','val']}\n",
    "\n",
    "dataset_sizes = {x:len(image_datasets[x])for x in ['train', 'val']}\n",
    "\n",
    "class_names = image_datasets['train'].classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Lee-PC\\anaconda3\\envs\\torch_projects\\lib\\site-packages\\torchvision\\models\\_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and may be removed in the future, \"\n",
      "c:\\Users\\Lee-PC\\anaconda3\\envs\\torch_projects\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# pre train model load\n",
    "\n",
    "from torchvision import models\n",
    "\n",
    "resnet = models.resnet50(pretrained=True)\n",
    "num_ftrs = resnet.fc.in_features\n",
    "resnet.fc = nn.Linear(num_ftrs, 33)\n",
    "resnet = resnet.to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer_ft = optim.Adam(filter(lambda p: p.requires_grad, resnet.parameters()), lr=0.001)\n",
    "\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "for child in resnet.children():\n",
    "    cnt +=1 \n",
    "    if cnt <6:\n",
    "        for param in child.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_resnet(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())  \n",
    "    best_acc = 0.0  \n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('-------------- epoch {} ----------------'.format(epoch+1)) \n",
    "        since = time.time()                                     \n",
    "        for phase in ['train', 'val']: \n",
    "            if phase == 'train': \n",
    "                model.train() \n",
    "            else:\n",
    "                model.eval()     \n",
    " \n",
    "            running_loss = 0.0  \n",
    "            running_corrects = 0  \n",
    " \n",
    "            \n",
    "            for inputs, labels in dataloaders[phase]: \n",
    "                inputs = inputs.to(DEVICE)  \n",
    "                labels = labels.to(DEVICE)  \n",
    "                \n",
    "                optimizer.zero_grad() \n",
    "                \n",
    "                with torch.set_grad_enabled(phase == 'train'):  \n",
    "                    outputs = model(inputs)  \n",
    "                    _, preds = torch.max(outputs, 1) \n",
    "                    loss = criterion(outputs, labels)  \n",
    "    \n",
    "                    if phase == 'train':   \n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    " \n",
    "                running_loss += loss.item() * inputs.size(0)  \n",
    "                running_corrects += torch.sum(preds == labels.data)  \n",
    "            if phase == 'train':  \n",
    "                scheduler.step()\n",
    " \n",
    "            epoch_loss = running_loss/dataset_sizes[phase]  \n",
    "            epoch_acc = running_corrects.double()/dataset_sizes[phase]  \n",
    " \n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc)) \n",
    " \n",
    "          \n",
    "            if phase == 'val' and epoch_acc > best_acc: \n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    " \n",
    "        time_elapsed = time.time() - since  \n",
    "        print('Completed in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    " \n",
    "    model.load_state_dict(best_model_wts) \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------- epoch 1 ----------------\n",
      "train Loss: 0.5098 Acc: 0.8454\n",
      "val Loss: 0.6032 Acc: 0.8678\n",
      "Completed in 1m 21s\n",
      "-------------- epoch 2 ----------------\n",
      "train Loss: 0.1402 Acc: 0.9526\n",
      "val Loss: 0.3137 Acc: 0.9109\n",
      "Completed in 1m 21s\n",
      "-------------- epoch 3 ----------------\n",
      "train Loss: 0.1156 Acc: 0.9635\n",
      "val Loss: 0.1370 Acc: 0.9580\n",
      "Completed in 1m 21s\n",
      "-------------- epoch 4 ----------------\n",
      "train Loss: 0.1026 Acc: 0.9656\n",
      "val Loss: 0.0877 Acc: 0.9692\n",
      "Completed in 1m 21s\n",
      "-------------- epoch 5 ----------------\n",
      "train Loss: 0.0689 Acc: 0.9760\n",
      "val Loss: 0.1210 Acc: 0.9645\n",
      "Completed in 1m 23s\n",
      "-------------- epoch 6 ----------------\n",
      "train Loss: 0.0701 Acc: 0.9773\n",
      "val Loss: 0.0978 Acc: 0.9676\n",
      "Completed in 1m 23s\n",
      "-------------- epoch 7 ----------------\n",
      "train Loss: 0.0889 Acc: 0.9712\n",
      "val Loss: 0.1043 Acc: 0.9707\n",
      "Completed in 1m 21s\n",
      "-------------- epoch 8 ----------------\n",
      "train Loss: 0.0563 Acc: 0.9827\n",
      "val Loss: 0.0610 Acc: 0.9815\n",
      "Completed in 1m 21s\n",
      "-------------- epoch 9 ----------------\n",
      "train Loss: 0.0382 Acc: 0.9873\n",
      "val Loss: 0.0549 Acc: 0.9830\n",
      "Completed in 1m 21s\n",
      "-------------- epoch 10 ----------------\n",
      "train Loss: 0.0347 Acc: 0.9883\n",
      "val Loss: 0.0410 Acc: 0.9857\n",
      "Completed in 1m 21s\n",
      "-------------- epoch 11 ----------------\n",
      "train Loss: 0.0308 Acc: 0.9897\n",
      "val Loss: 0.0419 Acc: 0.9842\n",
      "Completed in 1m 23s\n",
      "-------------- epoch 12 ----------------\n",
      "train Loss: 0.0275 Acc: 0.9915\n",
      "val Loss: 0.0339 Acc: 0.9861\n",
      "Completed in 1m 27s\n",
      "-------------- epoch 13 ----------------\n",
      "train Loss: 0.0251 Acc: 0.9908\n",
      "val Loss: 0.0353 Acc: 0.9857\n",
      "Completed in 1m 22s\n",
      "-------------- epoch 14 ----------------\n",
      "train Loss: 0.0265 Acc: 0.9910\n",
      "val Loss: 0.0370 Acc: 0.9880\n",
      "Completed in 1m 21s\n",
      "-------------- epoch 15 ----------------\n",
      "train Loss: 0.0235 Acc: 0.9919\n",
      "val Loss: 0.0375 Acc: 0.9865\n",
      "Completed in 1m 23s\n",
      "-------------- epoch 16 ----------------\n",
      "train Loss: 0.0208 Acc: 0.9929\n",
      "val Loss: 0.0342 Acc: 0.9896\n",
      "Completed in 1m 25s\n",
      "-------------- epoch 17 ----------------\n",
      "train Loss: 0.0237 Acc: 0.9922\n",
      "val Loss: 0.0348 Acc: 0.9892\n",
      "Completed in 1m 24s\n",
      "-------------- epoch 18 ----------------\n",
      "train Loss: 0.0227 Acc: 0.9931\n",
      "val Loss: 0.0319 Acc: 0.9896\n",
      "Completed in 1m 25s\n",
      "-------------- epoch 19 ----------------\n",
      "train Loss: 0.0183 Acc: 0.9937\n",
      "val Loss: 0.0330 Acc: 0.9884\n",
      "Completed in 1m 25s\n",
      "-------------- epoch 20 ----------------\n",
      "train Loss: 0.0218 Acc: 0.9920\n",
      "val Loss: 0.0315 Acc: 0.9877\n",
      "Completed in 1m 24s\n",
      "-------------- epoch 21 ----------------\n",
      "train Loss: 0.0196 Acc: 0.9937\n",
      "val Loss: 0.0378 Acc: 0.9857\n",
      "Completed in 1m 23s\n",
      "-------------- epoch 22 ----------------\n",
      "train Loss: 0.0196 Acc: 0.9931\n",
      "val Loss: 0.0340 Acc: 0.9854\n",
      "Completed in 1m 26s\n",
      "-------------- epoch 23 ----------------\n",
      "train Loss: 0.0175 Acc: 0.9938\n",
      "val Loss: 0.0356 Acc: 0.9873\n",
      "Completed in 1m 25s\n",
      "-------------- epoch 24 ----------------\n",
      "train Loss: 0.0193 Acc: 0.9933\n",
      "val Loss: 0.0349 Acc: 0.9892\n",
      "Completed in 1m 22s\n",
      "-------------- epoch 25 ----------------\n",
      "train Loss: 0.0205 Acc: 0.9928\n",
      "val Loss: 0.0329 Acc: 0.9900\n",
      "Completed in 1m 21s\n",
      "-------------- epoch 26 ----------------\n",
      "train Loss: 0.0190 Acc: 0.9936\n",
      "val Loss: 0.0333 Acc: 0.9884\n",
      "Completed in 1m 21s\n",
      "-------------- epoch 27 ----------------\n",
      "train Loss: 0.0193 Acc: 0.9938\n",
      "val Loss: 0.0380 Acc: 0.9873\n",
      "Completed in 1m 21s\n",
      "-------------- epoch 28 ----------------\n",
      "train Loss: 0.0184 Acc: 0.9945\n",
      "val Loss: 0.0340 Acc: 0.9900\n",
      "Completed in 1m 22s\n",
      "-------------- epoch 29 ----------------\n",
      "train Loss: 0.0186 Acc: 0.9931\n",
      "val Loss: 0.0321 Acc: 0.9880\n",
      "Completed in 1m 23s\n",
      "-------------- epoch 30 ----------------\n",
      "train Loss: 0.0174 Acc: 0.9942\n",
      "val Loss: 0.0347 Acc: 0.9888\n",
      "Completed in 1m 22s\n",
      "Best val Acc: 0.989977\n"
     ]
    }
   ],
   "source": [
    "model_resnet50 = train_resnet(resnet, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=EPOCH)\n",
    "torch.save(model_resnet50, 'resnet50.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_base = transforms.Compose([transforms.Resize([64,64]),\n",
    "                                    transforms.ToTensor()])\n",
    "test_base = ImageFolder(root='./splitted/test', transform=transform_base)\n",
    "test_loader_base = torch.utils.data.DataLoader(test_base, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline test acc:   4.780262143407865\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './resnet50.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15612\\1853217861.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'baseline test acc:  '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_accuracy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mresnet50\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./resnet50.pt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[0mresnet50\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mtest_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_accuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresnet50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_loader_resNet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Lee-PC\\anaconda3\\envs\\torch_projects\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[0;32m    769\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'encoding'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    770\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 771\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    772\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    773\u001b[0m             \u001b[1;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Lee-PC\\anaconda3\\envs\\torch_projects\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    268\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    269\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 270\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    271\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    272\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m'w'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Lee-PC\\anaconda3\\envs\\torch_projects\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 251\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    252\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './resnet50.pt'"
     ]
    }
   ],
   "source": [
    "transform_resNet = transforms.Compose([\n",
    "        transforms.Resize([64,64]),  \n",
    "        transforms.RandomCrop(52),  \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) \n",
    "    ])\n",
    "    \n",
    "test_resNet = ImageFolder(root='./splitted/test', transform=transform_resNet) \n",
    "test_loader_resNet = torch.utils.data.DataLoader(test_resNet, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n",
    "\n",
    "baseline=torch.load('baseline.pt') \n",
    "baseline.eval()  \n",
    "test_loss, test_accuracy = evaluate(baseline, test_loader_base)\n",
    "\n",
    "print('baseline test acc:  ', test_accuracy)\n",
    "\n",
    "resnet50=torch.load('./resnet50.pt') \n",
    "resnet50.eval()  \n",
    "test_loss, test_accuracy = evaluate(resnet50, test_loader_resNet)\n",
    "\n",
    "print('ResNet test acc:  ', test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2703f0f26327549540165d22fa3c581ed24ae2f5cd628e9f14f80960cd725dc4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
